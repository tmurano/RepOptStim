{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 14:14:12.750577: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "#for mat file \n",
    "import scipy.io as sio\n",
    "import seaborn as sb\n",
    "\n",
    "# import user defined functions\n",
    "from functions import *\n",
    "from utils import * \n",
    "\n",
    "# Import decoder functions\n",
    "from decoders import LSTMDecoder\n",
    "from decoders import SVRDecoder\n",
    "\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mat_file(res_all,result_file):\n",
    "\n",
    "    #save decoding result in .mat format\n",
    "    mat_name='./mat_data/{}'.format(result_file.split('/')[-2])\n",
    "    make_dir(mat_name)\n",
    "    for k in range(len(res_all)):\n",
    "        temp=res_all[k]\n",
    "        mn=temp['model']\n",
    "        dn=temp['data_name']\n",
    "        save_name='{}/{}_{}.mat'.format(mat_name,dn,mn)\n",
    "        sio.savemat(save_name,temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_not_flat(model):\n",
    "    \n",
    "    #train\n",
    "    model.fit(X_train, y_train)\n",
    "    #prediction\n",
    "    y_test_predicted=model.predict(X_test)\n",
    "    y_test_predicted=y_test_predicted * y_train_std + y_train_mean\n",
    "    #evaluation\n",
    "    corr,R2,mae,y_test_predicted_smooth,corr_smooth,R2_smooth,mae_smooth=calc(y_test,y_test_predicted)\n",
    "    #plot    \n",
    "    plot_all(y_test,y_test_predicted,out,data_name,model_name,target_name)\n",
    "        \n",
    "    res={'data_name':data_name,\n",
    "         'y_test':y_test,\n",
    "         'y_test_predicted':y_test_predicted,\n",
    "         'corr':corr,\n",
    "         'R2':R2,\n",
    "         'mae':mae,\n",
    "         'y_test_predicted_smooth':y_test_predicted_smooth,\n",
    "         'corr_smooth':corr_smooth,\n",
    "         'R2_smooth':R2_smooth,\n",
    "         'mae_smooth':mae_smooth,\n",
    "         'model':model_name,\n",
    "         'n_neuron':n_neuron}\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_th_list=[0]\n",
    "\n",
    "\n",
    "for th_velo, info_ind in product(velocity_th_list, [0]):\n",
    "    out='./Pos_vf_{}'.format(th_velo)\n",
    "        \n",
    "    make_dir(out)\n",
    "    result_dir=out#dir of decoding result         \n",
    "\n",
    "    result_file='{}/res_all.pkl'.format(result_dir)    \n",
    "    data_dir='./data/'\n",
    "    \n",
    "    #choose data you want to use for decoding\n",
    "    datalist_all=glob.glob('{}/*'.format(data_dir))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #######################\n",
    "    np.random.seed(100)\n",
    "\n",
    "    res_all=[]\n",
    "    pf_score_all=[]\n",
    "    for datalist_name in datalist_all:\n",
    "        print(datalist_name)\n",
    "        datalist=glob.glob(datalist_name + '/*')\n",
    "        data_name=datalist[0].split('/')[-2]\n",
    "\n",
    "        #load data\n",
    "        trace,spike,position,x_position,y_position,n_neuron,n_time,firing_rate,trace_smooth=data_load(datalist)      \n",
    "         \n",
    "\n",
    "        #thresholding by velocity\n",
    "        use_time_v=v_th(position,th_velo)\n",
    "       \n",
    "        trace=trace[use_time_v,:]\n",
    "        spike=spike[use_time_v,:]\n",
    "        position=position[use_time_v,:]\n",
    "        x_position=x_position[use_time_v]\n",
    "        y_position=y_position[use_time_v]\n",
    "        n_time=np.sum(use_time_v)\n",
    "        firing_rate=firing_rate[use_time_v,:]\n",
    "        trace_smooth=trace_smooth[use_time_v,:]\n",
    "        \n",
    "        ## Set what part of data should be part of the training/testing sets\n",
    "        train_range=[0, 0.5]\n",
    "        test_range=[0.5, 1.0]\n",
    "\n",
    "        train_set=np.arange(np.int(np.round(train_range[0]*n_time)),   np.int(np.round(train_range[1]*n_time)))\n",
    "        test_set=np.arange(np.int(np.round(test_range[0]*n_time)),   np.int(np.round(test_range[1]*n_time)))\n",
    "            \n",
    "        use_data=np.sum(spike[train_set,:],axis=0)>0#remove the neuron which dont activate at training\n",
    "        spike=spike[:,use_data]\n",
    "        trace=trace[:,use_data]\n",
    "        firing_rate=firing_rate[:,use_data]\n",
    "        trace_smooth=trace_smooth[:,use_data]\n",
    "        \n",
    "        n_neuron=spike.shape[1]\n",
    "        \n",
    "        \n",
    "        ######################## \n",
    "                 \n",
    "        \n",
    "        firing_rate=firing_rate-np.mean(firing_rate,axis=0,keepdims=True)           \n",
    "\n",
    "        X=get_spikes_with_history(firing_rate, 0, 0, 1)\n",
    "        X_flat=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))\n",
    "\n",
    "        \n",
    "        ########################\n",
    "        \n",
    "        target_name = ['x_position','y_position']          \n",
    "        y=position[:] \n",
    "        \n",
    "\n",
    "        ### 3C. Split into training / testing / testation sets\n",
    "\n",
    "        #Get training data\n",
    "        X_train=X[train_set,:,:]\n",
    "        X_flat_train=X_flat[train_set,:]\n",
    "        y_train=y[train_set,:]\n",
    "\n",
    "        #Get testing data\n",
    "        X_test=X[test_set,:,:]\n",
    "        X_flat_test=X_flat[test_set,:]\n",
    "        y_test=y[test_set,:]\n",
    "\n",
    "        #Z-score \"X\" inputs. \n",
    "        X_train_mean=np.nanmean(X_train,axis=0)\n",
    "        X_train_std=np.nanstd(X_train,axis=0)\n",
    "        X_train=(X_train-X_train_mean)/X_train_std\n",
    "        X_test=(X_test-X_train_mean)/X_train_std\n",
    "\n",
    "        #Z-score \"X_flat\" inputs. \n",
    "        X_flat_train_mean=np.nanmean(X_flat_train,axis=0)\n",
    "        X_flat_train_std=np.nanstd(X_flat_train,axis=0)\n",
    "        X_flat_train=(X_flat_train-X_flat_train_mean)/X_flat_train_std\n",
    "        X_flat_test=(X_flat_test-X_flat_train_mean)/X_flat_train_std\n",
    "\n",
    "        #Zero-center or zscore outputs\n",
    "        y_train_mean=np.mean(y_train,axis=0)\n",
    "        y_train_std=np.std(y_train,axis=0)\n",
    "        y_train_std_svr=np.std(y_train,axis=0)\n",
    "\n",
    "        y_train_std=y_train_std*0 +1\n",
    "        y_train_svr=(y_train-y_train_mean)/y_train_std_svr\n",
    "        y_train=(y_train-y_train_mean)/y_train_std\n",
    "\n",
    "        \n",
    "        ## 4. Run Decoders-----------------------------------------------------------------------\n",
    "            \n",
    "        # hyper parameters, for neural networks\n",
    "        epoch=10\n",
    "        unit=1000\n",
    "        dropout=0.1\n",
    "\n",
    "        model_name='lstm'\n",
    "        model=LSTMDecoder(units=unit,dropout=dropout,num_epochs=epoch)\n",
    "        res=model_eval_not_flat(model)\n",
    "        res_all.append(res)\n",
    "        print('{} : corr : {}'.format(model_name,res['corr_smooth'])) \n",
    "        \n",
    "        \n",
    "        plt.close('all')\n",
    "        print('----')\n",
    "        \n",
    " \n",
    "\n",
    "\n",
    "    with open('{}/res_all.pkl'.format(out),'wb') as f:\n",
    "        pickle.dump(res_all,f)\n",
    "\n",
    "    result={'dropout':dropout,\n",
    "            'unit':unit}\n",
    "    \n",
    "\n",
    "    with open('{}/parameters.pkl'.format(out),'wb') as f:\n",
    "        pickle.dump(result,f)\n",
    "        \n",
    "    ##Generate mat file\n",
    "    make_mat_file(res_all,result_file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
